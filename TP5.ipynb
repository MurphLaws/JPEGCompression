{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8cc3a9",
   "metadata": {},
   "source": [
    "# Graph Neural Networks for Recommendation Systems\n",
    "\n",
    "In this last TP, you are going to explore the use of Graph Neural Networks (GNNs) for Recommendation Systems.\n",
    "\n",
    "The dataset you'll use is the dataset MovieLens 100K. This dataset describes people's expressed preference for movies. Each preference is described as a tuple (user, movie, rating). \n",
    "\n",
    "Users and movies description are also added. On one side, users are defined by their age, sex, occupation and location. On the other side, movies are described by 19 categories, for which each movie can belong to one or more.\n",
    "\n",
    "This TP is two-fold. First part concerns the computation and the analysis of the dataset, and will mostly be treated in TD, and second part is about the use of GNNs for movie recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e1e20",
   "metadata": {},
   "source": [
    "## Part 1 : Dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d496f",
   "metadata": {},
   "source": [
    "### Libraries to include\n",
    "\n",
    "First of all, here are some libraries that you may need during this TP. Some of them are not mandatory, while other ones might not be used at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d01385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "!pip install gdown\n",
    "!gdown 1a7ZViLBLoCHKuLP8CSP_5uj0CTABqDaQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850bf3fd",
   "metadata": {},
   "source": [
    "### Setting the random seeds\n",
    "Used in order to fix the randomness of the experiments. It will be probably useless here, but it is always prefered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d46683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "\n",
    "RANDOM_STATE = 2021\n",
    "set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd62b73",
   "metadata": {},
   "source": [
    "### Read the dataset files using pandas\n",
    "\n",
    "You start to work here ! First of all, we need to read the data. For this, we are going to use pandas. We also use this stage to remove the unused fields.\n",
    "\n",
    "The unused fields are : \n",
    "- timestamp in ratings\n",
    "- movieId, movie title, release data, video release date and IMDb URL for movies\n",
    "- user id and zip code for users\n",
    "\n",
    "We also use this stage to ont hot encode and normalize our data. Movies categories are already one hot encoded, so we only need to encode the sex and occupation fields. For normalization, we'll normalize the age field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c331b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = {}\n",
    "    path = Path(path)\n",
    "    for filename in path.glob('*'):\n",
    "        if filename.suffix == '.data': \n",
    "            if filename.stem == 'u':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "                data = pd.read_csv(filename, sep='\\t', names=columns, engine='python', encoding='latin-1')\n",
    "                data.drop(columns = \"timestamp\", inplace= True)\n",
    "                files['ratings'] = data\n",
    "                \n",
    "        elif filename.suffix == '.item': \n",
    "            if filename.stem == 'u':\n",
    "                #TO DO\n",
    "                \n",
    "        elif filename.suffix == '.user': \n",
    "            if filename.stem == 'u':\n",
    "                #TO DO\n",
    "                \n",
    "    return files['ratings'], files['movies'], files[\"users\"]\n",
    "\n",
    "dataset = 'ml-100k'\n",
    "ratings, movies, users = read_data('data/'+dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b23d2",
   "metadata": {},
   "source": [
    "We can check what our ratings looks like : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d0d2f",
   "metadata": {},
   "source": [
    "We can do the same for movies and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae611e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ca63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328486a",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Let's take some time to analyze a bit our dataset. We can for example plot the the histogram of ratings. What do you observe ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"rating\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b0bc9",
   "metadata": {},
   "source": [
    "__Plot the different features and analyse them.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343885d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.sum().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO : plot the histogram for the users sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc730fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : plot the histogram for the users occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916315fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : plot the histogram for the users age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ced4d1",
   "metadata": {},
   "source": [
    "__Comment the different results you obtain. What kind of biases can you see ? How do you think those biases may perturb your model ? Do you think AI algorithms used also have biases ?__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee624c46",
   "metadata": {},
   "source": [
    "## Part 2 : Preparing the dataset \n",
    "\n",
    "No more analysis, now we want to apply machine learning models to recommend movies according to this dataset.\n",
    "\n",
    "First step is to prepare the dataset in order to make it usable by our models.\n",
    "\n",
    "The following function take the triples from ratings and convert it into X and y, where X are the pair user/movie, and y is the rating to predict.\n",
    "\n",
    "__Question : why did we modify the user index ?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('userId')['rating'].count()\n",
    "\n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "\n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "\n",
    "    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)\n",
    "\n",
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(X.head())\n",
    "print(y.head())\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6a686",
   "metadata": {},
   "source": [
    "Now, we have to separate our dataset into 3 parts : train, valid and test. By using the train_test_split() function from sklearn.model_selection, put 60% of the data in training, 20% of the data for validation, and 20% in test.\n",
    "\n",
    "Hint : the function divide the input into 2 splits...\n",
    "\n",
    "__Due to the number of entries, you might need to reduce the number of data in training and validation later (personnaly, i have 20% in train and 16% in valid).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : split the datasets\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid), 'test': (X_test, y_test)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid), 'test': len(X_test)}\n",
    "\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e2f1b8",
   "metadata": {},
   "source": [
    "Now we have our train, valid and test data, we need to compute the nodes of our graph. \n",
    "\n",
    "Here, our graph is a bit particular. It is a bipartite graph, where each part represent different entities (users and movies). In order to allow our models to differentiate users and movies, we'll add a new feature, which is going to be 0 if the node is a user, and 1 if the node is a movie. \n",
    "\n",
    "We end up by concatenating both matrices into a single one, our node matrix (H in the course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265478c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nodes(users, movies):\n",
    "    #TO DO\n",
    "    \n",
    "nodes = compute_nodes(users, movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3137b5",
   "metadata": {},
   "source": [
    "We also compute the corresponding adjacency matrix.\n",
    "\n",
    "__Question : why do we only use X_train to compute the adjacency matrix ?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix(X_train):\n",
    "    #TO DO\n",
    "    \n",
    "adjacency_matrix = compute_adjacency_matrix(X_train)\n",
    "full_adj = compute_adjacency_matrix(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37db4fb",
   "metadata": {},
   "source": [
    "__By using matplotlib, plot both adjacency_matrix. What do you observe ?__\n",
    "\n",
    "Once it is done, we won't use full_adj anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad650096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : plot the adjacency matrix and the full_adj matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f5374",
   "metadata": {},
   "source": [
    "## Part 3 : Training section\n",
    "\n",
    "We now have almost everything we need to train our model. First thing is to convert X_train, y_train, X_valid, y_valid, X_test and y_test into tensors (your job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : Transform your data into Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bea3f",
   "metadata": {},
   "source": [
    "Next step is to define a GNN Layer. Complete the following code in order to compute a simple GNN layer \\sigma(AHW+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=64): \n",
    "        super(GNN_layer,self).__init__()\n",
    "        \n",
    "        # TO DO :\n",
    "        # Attributes : input dim and output dim\n",
    "        \n",
    "        #Non linearity : ReLU for example\n",
    "        \n",
    "        #Layer : Linear layer\n",
    "\n",
    "        \n",
    "    def forward(self, nodes, adjacency):\n",
    "        # TO DO : \n",
    "        # AH multiplication\n",
    "        \n",
    "        # Apply Linear layer and non linearity\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea675c1",
   "metadata": {},
   "source": [
    "We now define our model. This model takes as input the nodes, the adjacency matrix, and the list of pair to predict X_(train/valid/test). It returns a value for each pair to predict. \n",
    "\n",
    "Complete the model in order to use one gnn layer you previously computed, and one linear layer to predict for each pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN_model, self).__init__()\n",
    "        # TO DO\n",
    "        # Attributes : intput dim, hidden dim (and output dim ?)\n",
    "        \n",
    "        # Add a GNN layer\n",
    "\n",
    "        \n",
    "        # Decision layer. Do we need an activation function ? If not, why ? If yes, which one ?\n",
    "\n",
    "        \n",
    "    def forward(self, nodes, adjacency, X):\n",
    "        # TO DO \n",
    "        # Apply GNN layer : \n",
    "        \n",
    "        \n",
    "        # Concatenate each pair of features for X. Given, because we are generous :)\n",
    "        node_pair = nodes[X].reshape((X.shape[0], 2*self.hidden_dim))\n",
    "        # Apply decision layer. Not given because we are not THAT generous \n",
    "        \n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ecf9a7",
   "metadata": {},
   "source": [
    "Time to train ! Start by transforming the adjacency matrix and the nodes matrix into tensors.\n",
    "\n",
    "Then, at each eapoch, make one step of training, and one step of validation, and save both losses into lists, in order to plot the training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81827a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 1000\n",
    "\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "\n",
    "nodes_tensor = torch.from_numpy(nodes).float().to(device)\n",
    "adjacency_matrix_tensor = torch.from_numpy(adjacency_matrix).float().to(device)\n",
    "\n",
    "# TO DO : Define your model, your loss and your optimizer\n",
    "\n",
    "for i in tqdm(range(nb_epochs)):\n",
    "    # TO DO \n",
    "    # As you've done in previous TP, apply your model on your TRAIN data, compute the loss and backpropagate it. \n",
    "    # Also add your loss to your list\n",
    "\n",
    "    # Now, apply your model on your VALID data. Compute your loss but do NOT backpropagate it. It is only for validation !\n",
    "    # Also add your lost to your list\n",
    "    \n",
    "# TO DO : Apply you model on TEST data and compute the Root Mean-Squared Error (outside the loop)\n",
    "\n",
    "\n",
    "# TO DO : plot both loss train and loss valid\n",
    "\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef69cc3",
   "metadata": {},
   "source": [
    "We now ant to improve our GNN model into a Vanilla GNN \\sigma((A+I)HW). No need to change our layer, we only need to change our adjacency matrix ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 1000\n",
    "\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "\n",
    "nodes_tensor = torch.from_numpy(nodes).float().to(device)\n",
    "# TO DO : modify your adjacency matrix tensor. The rest should not change from previous question\n",
    "\n",
    "# TO DO : Define your model, your loss and your optimizer\n",
    "\n",
    "for i in tqdm(range(nb_epochs)):\n",
    "    # TO DO \n",
    "    # As you've done in previous TP, apply your model on your TRAIN data, compute the loss and backpropagate it. \n",
    "    # Also add your loss to your list\n",
    "\n",
    "    # Now, apply your model on your VALID data. Compute your loss but do NOT backpropagate it. It is only for validation !\n",
    "    # Also add your lost to your list\n",
    "    \n",
    "# TO DO : Apply you model on TEST data and compute the Root Mean-Squared Error (outside the loop)\n",
    "\n",
    "\n",
    "# TO DO : plot both loss train and loss valid\n",
    "\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6806a",
   "metadata": {},
   "source": [
    "Finally, we want to implement the General GNN. This time, we have to modify our GNN layer in order to compute 2 linear layer : one for the adjacency matrix, and one for the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca232617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_layer_general(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=64): \n",
    "        super(GNN_layer_general,self).__init__()\n",
    "        # TO DO :  change the definition of your GNN Layer for General GNN \\sigma(AHW1 + IHW2)\n",
    "        # Attributes : input dim and output dim\n",
    "\n",
    "        \n",
    "        #Non linearity : ReLU for example\n",
    "\n",
    "        \n",
    "        #Layer : Linear layers\n",
    "\n",
    "    def forward(self, nodes, adjacency):\n",
    "        # TO DO : Compute your general GNN\n",
    "\n",
    "        return out_1+out_2 # <= this is an hint, but you can modify it if this helps you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5fd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN_model, self).__init__()\n",
    "        # TO DO : redefine your GNN model. SHould not change from before, but you need to reexecute it\n",
    "        \n",
    "        # Attributes : intput dim, hidden dim (and output dim ?)\n",
    "\n",
    "        # Add a GNN layer\n",
    "\n",
    "        \n",
    "        # Out layer. Do we need an activation function ? \n",
    "        \n",
    "        \n",
    "    def forward(self, nodes, adjacency, X):\n",
    "        # TO DO :\n",
    "        # Apply GNN layer : \n",
    "        \n",
    "        \n",
    "        # Concatenate each pair of features for X. Given, because we are generous :)\n",
    "        node_pair = nodes[X].reshape((X.shape[0], 2*self.hidden_dim))\n",
    "        # Apply decision layer. Not given because we are not THAT generous \n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 1000\n",
    "\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "\n",
    "nodes_tensor = torch.from_numpy(nodes).float().to(device)\n",
    "# TO DO : modify your adjacency matrix tensor. The rest should not change from previous question\n",
    "\n",
    "# TO DO : Define your model, your loss and your optimizer\n",
    "\n",
    "for i in tqdm(range(nb_epochs)):\n",
    "    # TO DO \n",
    "    # As you've done in previous TP, apply your model on your TRAIN data, compute the loss and backpropagate it. \n",
    "    # Also add your loss to your list\n",
    "\n",
    "    # Now, apply your model on your VALID data. Compute your loss but do NOT backpropagate it. It is only for validation !\n",
    "    # Also add your lost to your list\n",
    "    \n",
    "# TO DO : Apply you model on TEST data and compute the Root Mean-Squared Error (outside the loop)\n",
    "\n",
    "\n",
    "# TO DO : plot both loss train and loss valid\n",
    "\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfedaa18",
   "metadata": {},
   "source": [
    "__The end__\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
