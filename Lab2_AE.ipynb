{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"sUZCOMw1Gem5"},"source":["# Image Compression using AutoEncoders: a deep learning task\n","\n","Time : 8h\n","\n","\n","Your name(s): Manuela ARDILA, Nicolas LASSO\n","\n","Group: 2G1 - TP4"]},{"cell_type":"markdown","metadata":{"id":"mL9lLUJmpab6"},"source":["# Goal of this lab\n","\n","* Get to know Deep Learning and PyTorch framework\n","* Learn how to compress images using AutoEncoders\n","* Understand the differences between JPEG compression and deep learning-based compression\n","* Know how to read and use code given by someone else\n"]},{"cell_type":"markdown","metadata":{"id":"NN6NsODyqGEw"},"source":["# Outline\n","* 0 - Introduction and documents reading\n","* 1 - Discovering a dataset\n","* 2 - Creating a Model\n","* 3 - Training the Model\n","* 4 - Testing the Model\n","* 5 - Reporting the findings"]},{"cell_type":"markdown","metadata":{"id":"qc8OA-MQ6n09"},"source":["# Disclaimer\n","\n","We will refer to :\n","- AutoEncoder as AE\n","- Multi Layer Perceptron as MLP\n","\n","Please make sure that your environment has a GPU. For that, go to: \n","* Execution\n","* Modifier le type d'execution\n","* Accelerateur mat√©riel: GPU\n","\n","\n","We advice you to keep in an Excel Sheet or somewhere else the important values, for different parameters, that you will compute during this lab (accuracies, losses...)"]},{"cell_type":"markdown","metadata":{"id":"fup2OihC7Kkv"},"source":["# 0 - Introduction\n","\n","We have seen JPEG compression, a general algorithm that can compress any image. Let's first see your understanding of the JPEG algorithm :\n","* What are the component in the encoding part of the algorithm ?\n","* What are the component in the decoding part of the algorithm ?\n","* Is the down-sampling phase of JPEG linear ? (i.e : in a y = ax+b form)\n","* Is it a lossless compression algorithm ?"]},{"cell_type":"markdown","metadata":{"id":"MmPz4EM86n09"},"source":["## a- Your mission: AI for the Win\n","\n","Hi there, we are a Big Company and we need your help as a data scientist. We have some images to compress. We tried the JPEG algorithm but we also want to try some modern methods based on deep learning (DL). \n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"315B4XDj9ln-"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[ 1  2  3]\n","  [ 4  5  6]\n","  [ 7  8  9]]\n","\n"," [[10 11 12]\n","  [13 14 15]\n","  [16 17 18]]]\n","(2, 3, 3)\n"]}],"source":["%matplotlib inline\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader\n","from torch.utils.data import random_split\n","from torchvision.datasets import MNIST, CIFAR10\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import skimage\n","from skimage import io\n","import numpy as np\n","#from google.colab.patches import cv2_imshow\n","import cv2\n","import torchvision\n","import torchvision.transforms as transform\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2AKbYlYjUiw8"},"source":["## b - Reading the documents \n","\n","**Very important: take 20-25 minutes to read the following documentation to understand the basic.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y1iy5iLS6n0_"},"source":["### i - What is deep learning ?\n","\n","Deep Learning is a branch of AI where you **teach a Model** a certain **task** using a **Dataset**. The model or a neural network is built by multiple consecutive **layers** of neuron-like units, remotely based on neurons in the human brain. Typically, many consecutive layers are used, that is why it is referred to as deep learning. In those layers, each neuron has several **parameters** (**weights**) that are updated during **training** by minimizing a **loss** (error) function, using **Stochastic Gradient Descent**. Besides the model parameters, to be found using a dataset, there are also **hyperparameters** that you have to tune by yourself, for example, how many layers used in your model, how many neurons per layer,.... The Model infers a prediction from an **input**. In fact, a Deep Neural Network can be seen as a complex function ${f}$ that maps the input data to a learned space from the Dataset. \n","\n","Note the bold words. These are the important things you need to understand about Deep Learning. "]},{"cell_type":"markdown","metadata":{"id":"awLz7ozJ6n1A"},"source":["### ii- Generalities on AutoEncoder\n","\n","<img src = \"https://blog.keras.io/img/ae/autoencoder_schema.jpg\">\n","\n","\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) **data-specific**, 2) **lossy**, and 3) **learned** automatically from examples rather than engineered by a human. In almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n","\n","1) Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. \n","\n","2) Autoencoders are lossy.\n","\n","3) Autoencoders are learned automatically from data examples.\n","\n","To build an autoencoder, you need three things: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation. In fact, we can look at the model as a big function :\n","\n","* Encoder : This part compresses the input image to a compressed version of it, where $f(x)= z$, where $x$ is the input image, and $z$ the compressed representation of it.\n","* Decoder : This part of the model decompresses the compressed representation $z$ to the decompressed image $\\tilde{x}$, in other terms we have a function $g$ where, $\\tilde{x} = g(z)$\n","* AutoEncoder : by stacking the Encoder and the Decoder, and as we want $\\tilde{x} = x$ (a.k.a the reconstructed image as similar as the input image), we can rewrite the AE as $\\tilde{x} = g(z) = g\\circ f(x)=  x$ where $g= f^{-1}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qhcd3QvKhXdj"},"source":["### iii - Layers\n","\n","A Layer is an important part of a model. In fact, it is the key element of a DL model. A Layer is a structure that takes information from one layer to pass them to the next layer. In a DL network, each layer extracts some kinds of features. "]},{"cell_type":"markdown","metadata":{"id":"Z7pUFCRvhYuE"},"source":["### iv -  Activation functions\n","\n","An (non-linear) activation function decides the output of a neuron. It is the equivalent of the excitation threshold for which a neuron reacts or not.\n","\n","**Hint**: ReLU is often used as activation function in hidden layers.\n","\n","<img src = \"https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png\" height = 200>"]},{"cell_type":"markdown","metadata":{"id":"_cQgt2SkteEw"},"source":["# 1- Exploring the data"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uqws4UbZ6n1B"},"source":["## Dataset\n","\n","<img src=\"https://labelyourdata.com/img/article-illustrations/splitting_data.png\" height=200>\n","\n","\n","When training  a DL model, we use a Dataset. The model uses the data to learn something for a task. We usually divide the data into Training, Validation, Test sets.\n","- Training set is used to train the Model (i.e., to find the parameters of Model).\n","- Validation set is used to watch the Model's training (to verify whether the training procedure goes well).\n","- Test set is used to evaluate the performance of the Model (in our case, to measure if the model compresses and decompresses well new images).\n","\n","### ***Meme score: 6/10***\n","\n","<img src=\"https://i.imgflip.com/653bu2.jpg\" height=400>\n","\n","**In our case of AE, we do not need the label of image. Our method is an unsupervised algorithm.**"]},{"cell_type":"markdown","metadata":{"id":"TRKH2GO8PPKS"},"source":["### a - The first one : the training set\n","\n","- What is the size of the training dataset?\n","- What are the elements available in one piece of data? (image, label)\n","- What is the shape of one piece of data?\n","- What is the type of one piece of data?\n","- Plot few elements of the dataset using Matplotlib."]},{"cell_type":"code","execution_count":67,"metadata":{"id":"SOdefqiw6n1B"},"outputs":[{"name":"stdout","output_type":"stream","text":["The data is composed of 60 000 images of 28x28 pixels\n","47.04 mb\n","60000\n","torch.Size([28, 28])\n","tensor([5, 0, 4])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 1000x1000 with 3 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# TODO: Load MNIST Train Dataset from TorchVision\n","\n","#how to use the MNIST function from torchvision to load the MNIST train dataset\n","\n","dataset = MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n","\n","dataset\n","\n","# TODO: What's the size of the Dataset ?\n","def get_size_in_bytes(tensor):\n","    return tensor.element_size() * tensor.nelement()\n","print(\"The data is composed of 60 000 images of 28x28 pixels\")\n","print(get_size_in_bytes(dataset.data)/1000000, \"mb\")\n","# end TODO\n","\n","\n","# TODO: Retrieve one element of the Dataset ? What is the shape of one piece of Data ?\n","\n","size_of_dataset =  len(dataset)\n","print(size_of_dataset)\n","data = dataset.data\n","print(data[0].shape) # Question : Why is there [0] ?\n","#We put the [0] because the dataset.data attribute is a list of images represented as tensor\n","# end TODO\n","\n","# TODO: Plot the retrieved Data\n","#plot the first 3 elements on data\n","\n","\n","\n","print(dataset.targets[0:3])\n","plt.figure(figsize=(10, 10))\n","for i in range(3):\n","    plt.subplot(1, 3, i+1)\n","    plt.imshow(data[i], cmap='gray')\n","    plt.axis('off')\n","plt.show()\n","\n","\n","\n","# end TODO\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IU_I7BipPEto"},"source":["### b - The second one: the test dataset"]},{"cell_type":"markdown","metadata":{"id":"KsqbBFiz6n1C"},"source":["As you can see in the above code, there's a \"train\" attribute of the MNIST class. When it is set to True, you are loading the training dataset. Therefore, in the following code, change it to false to load the test dataset."]},{"cell_type":"code","execution_count":58,"metadata":{"id":"YVnRxxaa6n1C"},"outputs":[],"source":["# TODO : Load the test dataset. Inspire yourself from the mnist_train dataset loading\n","\n","mnist_test = MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n","\n","# end TODO"]},{"cell_type":"markdown","metadata":{"id":"y77InD1eSuQF"},"source":["Let's verify that the data in the test dataset are in the same style of the train Dataset :    \n","- Plot few data from the test dataset with its label.\n","- Are the data similar? Are the labels similar?"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"ZGl0JVb9S75z"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([28, 28])\n","tensor([7, 2, 1])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 1000x1000 with 3 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# TODO: What's the size of the Dataset ?\n","# TODO: Retrieve one element of the Dataset ? What is the shape of one piece of Data ? \n","# TODO: Plot the retrieved Data\n","\n","size_of_dataset = len(mnist_test)\n","test_data = mnist_test.data\n","print(data[0].shape)\n","\n","print(mnist_test.targets[0:3])\n","\n","plt.figure(figsize=(10, 10))\n","for i in range(3):\n","    plt.subplot(1, 3, i+1)\n","    plt.imshow(test_data[i], cmap='gray')\n","    plt.axis('off')\n","plt.show()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6x4LBC7HUk8S"},"source":["### c - The third one: creating the Validation dataset"]},{"cell_type":"markdown","metadata":{"id":"OSbPGhqP6n1D"},"source":["Besides the training and testing sets, we would have a validation set. The validation set allows us to follow the training of model or in other words verify whether the training procedure goes well or not. \n","\n","In the next codes,\n","- What do 55000 and 5000 mean ?\n","- Determine the split value of the training set to create the validation dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HiGz4sBr6n1D"},"outputs":[],"source":["# Questions : what does 55000 and 5000 mean ? Hint: look at the Dataset length and determine the split value\n","mnist_train, mnist_val = random_split(dataset, [55000, 5000])"]},{"cell_type":"markdown","metadata":{"id":"Blh2vwZu6n1D"},"source":["### d - Creating the Dataloaders\n","\n","The dataset returns one element or item at a time. In DL, we prefer sending many items at the same time to the model. We form BATCH of Data using a DataLoader. Dataloader is an iterable over the dataset. It means that the Dataloader will form batches of Data for you and fetch them when you loop through it.\n","\n","- Create a DataLoader for your Training, Validation and Test Dataset\n","- What is the drop_last attribute ?\n","\n","More information on dataloader : https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EM2xVJQ6n1D"},"outputs":[],"source":["train_loader = DataLoader(mnist_train, batch_size=128,drop_last =True)\n","val_loader = ...\n","test_loader = ..."]},{"cell_type":"markdown","metadata":{"id":"eHI5FPtkbiY_"},"source":["- Is there enough data?\n","- Are they easily accessible?\n","- Are they correctly labeled?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"utu6siUB6n1D"},"source":["# 2 - Creating and training the model: AutoEncoder\n","\n","We saw what the data was and created our datasets. We need now to fullfil our mission and we need a model.\n","\n","<img src=\"https://i.imgflip.com/640uob.jpg\" height=300>\n","\n","We are going to explore the path of AutoEncoder! Let's write some readable codes. Our code must be modulable and easy to read. We should try two types of AutoEncoders :    \n","- MLP Style\n","- Conv Style\n","\n","\n","Use PyTorch and Create Modulable and Stackable Models that inherits from nn.Module.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vjUHGbOi6n1E"},"source":["## a - MLP Style : Exploring the Neurons\n","\n","<img src='https://www.researchgate.net/publication/344394387/figure/fig1/AS:974657746399232@1609387923440/Figure-Computational-Schematics-of-the-MLP-and-the-autoencoder.png'>\n","\n","We will first try a MLP AE.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UvnEBg0ahMa1"},"source":["#### Creating a Model in PyTorch\n","\n","Creating a model in PyTorch is simple. A PyTorch is an object that inherits from nn.Module. The pseudo-code is the following :     \n","\n","```\n","class Model(nn.Module):\n","  def __init__(self,...):\n","    \"\"\"\"\n","    Define the model. You can put the input size as a parameter if needed..\n","    \"\"\"\"\n","    super().__init__() # to init the main class\n","    self.layers = ... # defining the model : could be Conv2d, Linear, RNN, LSTM\n","\n","\n","  def forward(self,x):\n","    \"\"\"\n","    The input x is forwarded through the neural net. \n","    \"\"\"\n","    output = self.layers(x)\n","    return output\n","\n","  # Other methods go down\n","```\n","\n","More informations : https://pytorch.org/docs/stable/nn.html\n"]},{"cell_type":"markdown","metadata":{"id":"T730pWdwLRXh"},"source":["### i - Correcting the Mistakes\n","\n","First, we will try a really simple model :\n","* an Input Dense layer\n","* a latent space\n","* an Output Dense layer\n","\n","\n","We have received some codes from the other members of the Deep Learning Engineering Team. There are lots of mistakes... Let's correct them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxvGXm9rLx2W"},"outputs":[],"source":["# TODO : Correct the Following Class. \n","# Hint : Look at the input, output size, the activations, how the data is forwarded ...\n","\n","class AutoEncoder_MLP(nn.Module):\n","  def __init__(self, input_size, compressed_space_size):\n","    \"\"\"\n","    The model is an Input Layer, a Hidden Layer and an Output layer \n","    \"\"\"\n","    super().__init__() \n","    # TODO : Init the class attributes thanks to the arguments of the init methods\n","    self.input_size = 42\n","    self.output_size = 'ouioui le sens de la vie'\n","    self.compressed_space_size = -1\n","    # TODO : Correct the mistakes from the model\n","    self.input = nn.Sequential(nn.Linear(self.input_size, self.compressed_space_size),\n","                               nn.Sigmoid())\n","    self.output = nn.Sequential(nn.Linear(self.output_size,self.compressed_space_size ),\n","                               nn.ReLU())\n","    # end TODO: there are mistakes in all these 5 above lines\n","\n","  def forward(self,x):\n","    \"\"\"\n","    The input x is forwarded through the neural net. \n","    \"\"\"\n","    # TODO : Correct the mistakes\n","    compressed_image = self.input(x)\n","    decompressed_image = ...\n","    return decompressed_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSb86xStr3gK"},"outputs":[],"source":["# TODO : Create an Instance of the Model by calling the Class with the correct values\n","\n","model = AutoEncoder_MLP(input_size = ...,\n","                        compressed_space_size= ...)\n","\n","# TODO : Print the model.\n","..."]},{"cell_type":"markdown","metadata":{"id":"7N8KQNJy6n1F"},"source":["## b - Training\n","\n","We can train the model. We have a Model and a Dataset. We need few more things..\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5MqMXbQw_1Kr"},"source":["### i - Loss function\n","\n","The loss function must tell us how far our predictions are from the true labels. \n","This could be done by comparing the distributions of two input data, or by directly comparing the data using some distance metrics.\n","\n","We are reconstructing an image from its compressed version and we want the reconstructed image as similar as possible to the original image. i.e $\\tilde{x} = x$\n","\n","<img src=\"https://i.imgflip.com/6946oe.jpg\" height=400>\n"]},{"cell_type":"markdown","metadata":{"id":"cjJwaIhlVrWD"},"source":["\n","- How can you calculate the similarity between two vectors?\n","- What type(s) of loss function do you know that calculates the **distance** between two inputs?\n","\n","More information: https://pytorch.org/docs/stable/nn.html#loss-functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Am2fuauUBoUd"},"outputs":[],"source":["# TODO : Delete the uncorrect loss \n","criterion = nn.MSELoss() or nn.BCELoss() "]},{"cell_type":"markdown","metadata":{"id":"vdJa2rgV___i"},"source":["### ii - An Optimizer\n","\n","<img src=\"https://i.imgflip.com/640sfs.jpg\" height= 400>\n","\n","We need to gradually update the weights of the model. In fact, we perform the **gradient descent** to recalculate the weights of each layer regarding the model's predictions. The optimizer will search for an Optimum. It needs a step to perform this research. This step is called the **learning rate**. The learning rate has an important effect on the learning phase (duration, convergence,...).\n","Think of the **gradient descent** as you trying to answer an exercise : the first time you won't understand, the second time you'll suceed more, and so on...\n","\n","<img src=https://miro.medium.com/max/918/0*uIa_Dz3czXO5iWyI. height =300>\n","\n","In this case, we will use Adam Optimizer (it is efficient). Don't hesitate to have a look at the other optimizers. \n","\n","More information: https://pytorch.org/docs/stable/optim.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lykHsQ-HrMz"},"outputs":[],"source":["# TODO : define a small learning rate\n","learning_rate =  \n","# TODO : load the Adam optimizer in the optimizer variable\n","optimizer = torch.optim.Adam(model.parameters(), lr=....)\n"]},{"cell_type":"markdown","metadata":{"id":"9sVerRhLIdpI"},"source":["### iii - Training\n","\n","Please use GPU to accelerate this phase.\n","\n","Training a model consists of the following loop :\n","* Sending Data through the model to obtain Predictions\n","* Computing the Loss \n","* Backwarding the Loss using Gradients \n","* Logging the losses and accuracies (if exists) (Optional)\n","\n","The number of epochs is a hyperparameter that defines the number of time the learning algorithm will work through the entire training dataset.\n","\n","\n","\n","* Create your training and validation loop to train your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_9nrxJj7Bj6"},"outputs":[],"source":["# Pre Defined and Useful variables\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' # To send to the gpu\n","mini_batches_print = 10 # To print every 10 mini batches\n","\n","# TODO : Send the model to the device using .to\n","net = \n","\n","# TODO : Define your number of epochs\n","\n","num_epochs = ...\n","\n","loss_train , loss_val = [], []\n","for epoch in range(num_epochs) : \n","    running_loss_t,running_loss_v = 0.0, 0.0\n","    # TODO : Create your Training Loop\n","    for i, data in enumerate(train_loader, 0): \n","        # TODO : load the data into two variables\n","        image, label = ...\n","\n","        # TODO : reshape the input image using .view() so that it fits the input layers neuron numbers.\n","        # Don't forget the Batch Size, the 1rst dimension must always be the Batch Size\n","        image_reshaped, label = ...\n","        optimizer.zero_grad()\n","\n","        # TODO : send the image to the model\n","        outputs = net(...)\n","\n","        # TODO/Questions : Do we need to reshape the input image ? If yes, reshape the image\n","        outputs = ...\n","\n","        # TODO : Compute the loss. Don't forget to send the image to the device\n","        loss = criterion(... ,... )\n","\n","        loss.backward()\n","        optimizer.step()\n","        running_loss_t += loss.item()\n","        if i == mini_batches_print :\n","          running_loss_t= running_loss_t/mini_batches_print  \n","          print('training loss is :',running_loss_t)\n","          loss_train.append(running_loss_t)  \n","\n","    # TODO : Create your Validation Loop\n","    with torch.no_grad():\n","      for i, data in enumerate(val_loader, 0):    \n","        # TODO : Do the same as the Train loop but delete everything related to weight update (optimizer, loss backwards ...)\n","        image, label = data[0].view(data[0].shape[0],-1).to(device), data[1]\n","        outputs = ...\n","        outputs = ...\n","        loss = ...\n","        optimizer.zero_grad() # Keep or not ?\n","        loss.backward() # Keep or not ?\n","\n","        running_loss_v += loss.item()\n","        if i == mini_batches_print :\n","            running_loss_v= running_loss_v/mini_batches_print \n","            print('validation loss is :',running_loss_v)\n","            loss_val.append(running_loss_v)\n","      "]},{"cell_type":"markdown","metadata":{"id":"KP7wfVDTVNn0"},"source":["### iv - Did it learn something ?\n","\n","As you might see, we logged into two lists (train_loss, val_loss) losses computed during training. Let's plot them (do not forget to put titles and axis)\n","- How can you tell that the training is over?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myd0mSIkVc7v"},"outputs":[],"source":["# TODO : plot the train and val loss on the same graph using matplotlib.pyplot.\n","# Always put legend on your graphs\n","\n","plt.plot(...)\n","plt.plot(...)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"6rFLeJkes8Ll"},"source":["## c - Testing the compression\n","\n","We already trained our model, let's test it on the testing set. We received an email from the senior Data Scientist :\n","\n","\n","---\n","\n"," üîä üîä üîä **Message from the Senior Data Scientist** üîä üîä üîä\n","\n","Hi, this is C. the Senior Data Scientist\n","\n","I heard that you've trained your model. Let's test it.\n","Take the test dataloader, iterate through it and send the test data to the model. We need to check how similar to the original image the decompressed image is..\n","Do not forget to delete all gradient calculation, it takes time and space for nothing. \n","\n","\n","---\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FISqPpOGuGHC"},"source":["* Why must we not compute the gradients for the testing step?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AB-kxbEtIbt"},"outputs":[],"source":["def imshow(img,name= 'GT'):\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.title(name)\n","    plt.show()\n","\n","# TODO : form your testing loop. Is it different than the validation loop?\n","with torch.no_grad():\n","  losses = 0\n","  for i, data in enumerate(test_loader, 0):\n","    image, label = .....\n","    .... \n","    .... \n","    ....\n","\n","# Plot the last batch\n","imshow(torchvision.utils.make_grid(outputs.detach().cpu()),'Pred')\n","imshow(torchvision.utils.make_grid(data[0]),'GT')\n","\n","# TODO : Print the difference in decompression and write it somewhere\n","print('The difference between the Real Images and the Decompressed Images is: ',...)"]},{"cell_type":"markdown","metadata":{"id":"jak6Xd8h6n1F"},"source":["<img src=\"https://i.imgflip.com/64elyi.jpg\" height=200>\n","\n","Let's see the effect of the compression: \n","* Change the latent_size to different values and compare the compression differences. \n","\n","For example, try 512, 128, 16, 1.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRMKAqZdzrVU"},"outputs":[],"source":["# TODO : Change your model, Test for differents size of Compressed Space Size. We advice you to try 1, 128, 512\n","model = AutoEncoder_MLP(28*28,...)\n","\n","# TODO : Train it. Can we copy paste previous things ?\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","net = model.to(device)\n","criterion = ...\n","learning_rate = ...\n","optimizer = ...\n","num_epochs = ...\n","\n","# TODO : Train it. Can we copy paste previous things ?\n","for epoch in range(num_epochs) : \n","    running_loss_t,running_loss_v = 0.0, 0.0\n","    # Train Loop\n","    for i, data in enumerate(train_loader, 0): \n","      ....\n","\n","\n","    # Validation Loop\n","    with torch.no_grad():\n","      for i, data in enumerate(val_loader, 0):  \n","        ...\n","\n","# TODO : Test the Trained Model. Can we copy paste previous stuff ?\n","\n","with torch.no_grad():\n","  running_loss = []\n","  for i, data in enumerate(test_loader, 0):  \n","    ...  \n","\n","# Plot the last batch\n","imshow(torchvision.utils.make_grid(outputs.detach().cpu()),'Pred')\n","imshow(torchvision.utils.make_grid(data[0]),'GT')\n","\n","# TODO : Print the reconstruction error over the test dataset\n","\n","print( 'The decompression of the Encoded Test Dataset has an reconstruction error of: ', ....)\n"]},{"cell_type":"markdown","metadata":{"id":"d4dirY6mE_Sp"},"source":["## d - Testing on other type of data\n","\n","We can say that the model works pretty well on the test data. Let's test it on other kind of images.\n","\n","* Write the code to test the compression on **images** taken from the internet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8GcBFKAFCqZ"},"outputs":[],"source":["# TODO : Test some images from the internet to see the compression effect \n","image_filename = # TODO: give a URL's image \n","image_numpy = cv2.cvtColor(skimage.io.imread(image_filename ),cv2.COLOR_BGR2GRAY)\n","transform=transforms.Compose([transforms.ToTensor(),\n","                              transforms.Resize((28,28))])\n","\n","# TODO : Transform the images and add a dimension for the batch size using unsqueeze\n","image = transform(...).unsqueeze(0)\n","\n","# TODO : Send the model to the model and process the prediction. Don't forget \n","# the resizes\n","pred = model(...)\n","pred = ...\n","\n","# TODO : Plot dem results'. Don't forget to detach and send to the gpu the data\n","# using .detach().cpu()\n","\n","fig,axarr = plt.subplots(1, 2)\n","axarr[0].imshow(...)\n","...\n"]},{"cell_type":"markdown","metadata":{"id":"0UhnIMewpF5K"},"source":["* What can you say ?"]},{"cell_type":"markdown","metadata":{"id":"J8Ma8YUNE3So"},"source":["# 3- Write your report\n","\n","* Sum up all your understanding on this subject in your report.\n","\n","You should explain:\n","* The dataset you used for training,\n","* The tests you did,\n","* The models you tested,\n","* The results you had,\n","* The explanation of the results (why it works, why it doesn't),\n","* How can we compute the compression rate of the model? \n","* Do we need to consider the model's size?"]},{"cell_type":"markdown","metadata":{"id":"i1kpRMLuU9_n"},"source":["# 4 - Convolutional layer style : seeing a region\n","\n","<img src=\"https://miro.medium.com/max/1838/1*LSYNW5m3TN7xRX61BZhoZA.png\" height = 300>\n","\n","---\n"," üîä üîä üîä **Message from the senior data scientist** üîä üîä üîä\n","\n","Hello,\n","\n","Good work! We need now to try another type of model.\n","Please consider Conv2d layers. \n","\n","Here you are some explanations: Convolutional Layers are filters that \"scans\" the input image in order to extract features. These filters extract features by looking at the region they're on.\n","\n","I sent you some classes that you should use in your code. \n","\n","C.\n","\n","<img src=\"https://i.imgflip.com/65b89l.jpg\" height=300>\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ibihGUzZ-C00"},"source":["**Take 5 minutes to learn more about convolution with the following link:**\n","\n","* https://ezyang.github.io/convolution-visualizer/\n","\n","Questions:    \n","* What is the stride parameter?\n","* What is the padding parameter?\n","* What will change on the output when increasing the Kernel Size?\n","\n","**Receptive field: are the pixels seen by filter kernel.**\n"]},{"cell_type":"markdown","metadata":{"id":"6v_cYtuufVur"},"source":["## a - Model definition\n","\n","Have a look at the model sent by the senior data scientist."]},{"cell_type":"markdown","metadata":{"id":"FiDIW6xzfURZ"},"source":["### i - Submodules"]},{"cell_type":"markdown","metadata":{"id":"6aR_6-5AfURZ"},"source":["#### Conv Down\n","\n","ConvDown is used to reduce the image size. A convolution on the input image is done using a kernel (the coefficients of kernel are weights to be found). In fact, it is used to extract interesting features of image. Our ConvDown Model will be composed of two layers:     \n","* Conv2d layer\n","* Non linearity (ReLU)\n","\n","<img src=\"https://www.jeremyjordan.me/content/images/2017/07/no_padding_no_strides.gif\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30AE171_fURZ"},"outputs":[],"source":["class ConvDown(nn.Module):\n","    \"\"\"\n","    This class takes as input the channels and returns a feature map \n","    for the given output channel. \n","    It indeed applies ReLU to it\n","    ConvDown stacks a Conv2d layer with an ReLU Activation \n","\n","    \"\"\"\n","    def __init__(self, input_channel, output_channel, kernel_size = 3):\n","        super().__init__()\n","        self.input_channel = input_channel\n","        self.output_channel = output_channel\n","        self.kernel_size = kernel_size\n","        self.model = nn.Sequential(nn.Conv2d(self.input_channel, self. output_channel, kernel_size =self.kernel_size ),\n","                                    nn.ReLU())\n","                                    \n","    def forward(self,x):\n","        # TODO :  Send the data through the model and return the output\n","        output = ...\n","        return"]},{"cell_type":"markdown","metadata":{"id":"RTj9S6BdfURZ"},"source":["#### Conv Up\n","\n","ConvUp is used to decompress the input image. In fact, it uses extracted features to reconstruct output feature map.\n","\n","* Is it possible to increase output size map using Conv2d layers?\n","\n","We can use also ConvTranpose2D layers, that use **transpose convolution** over an input image. These ConvTranspose layers learn to upsample the input images.\n","\n","<img src=\"https://miro.medium.com/max/1400/1*HnxnJDq-IgsSS0q3Lut4xA.gif\" height=200>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iup_9vP8fURZ"},"outputs":[],"source":["class ConvUp(nn.Module):\n","    \"\"\"\n","    ConvUp stacks a Conv2d layer with an Activation \n","    If output is True : the Activation is Sigmoid\n","    If output is False : the Activation is ReLU\n","    \"\"\"\n","    def __init__(self, input_channel, output_channel, kernel_size = 3 , output = True):\n","        super().__init__()\n","        self.input_channel = input_channel\n","        self.output_channel = output_channel\n","        self.kernel_size = kernel_size\n","        self.output = output\n","        self.model = nn.Sequential(nn.ConvTranspose2d(self.input_channel, self. output_channel, kernel_size =self.kernel_size ),\n","                                    nn.ReLU()) if output is False else nn.Sequential(nn.ConvTranspose2d(self.input_channel, self. output_channel, kernel_size =self.kernel_size ),\n","                                    nn.Sigmoid())\n","        \n","    def forward(self,x):        \n","      # TODO :  Send the data through the model and return the output\n","      output = ...\n","      return"]},{"cell_type":"markdown","metadata":{"id":"xKSienu_fURZ"},"source":["### ii - Modules : the wrappers"]},{"cell_type":"markdown","metadata":{"id":"add5ul-BfURZ"},"source":["#### Encoder\n","\n","The encoder stacks multiple ConvDown to compress and extract features.\n","\n","<img src=\"https://i.imgflip.com/65bqe0.jpg\" height=300>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJg8gNNPfURZ"},"outputs":[],"source":["class Encoder(nn.Module):\n","    \"\"\"\n","    The encoder stacks multiple ConvDown to compress and extract features\n","    For the moment, we just keep one ConvDown layer\n","\n","    \"\"\"\n","    def __init__(self,input_channel, output_channel, kernel_size = 3):\n","        super().__init__()\n","        self.input_channel = input_channel\n","        self.output_channel = output_channel\n","        self.kernel_size = kernel_size\n","        self.model = nn.Sequential( ConvDown(self.input_channel, self.output_channel,self.kernel_size))\n","                    \n","    def forward(self,x):\n","      # TODO :  Send the data through the model and return the output\n","      output = ..."]},{"cell_type":"markdown","metadata":{"id":"-Ty_eE8ufURa"},"source":["#### Decoder\n","\n","The decoder stacks multiple ConvUp to decompress and upsamples the input.\n","\n","<img src=\"https://i.imgflip.com/65br5o.jpg\" height=300>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOIjpP5JfURa"},"outputs":[],"source":["# TODO : Find the error in the initialization and correct it\n","class Decoder(nn.Module):\n","    \"\"\"\n","    The Decoder stacks multiple ConvUp to upsample and reconstruct from the input\n","    another feature map\n","    For the moment, we just keep one ConvDown layer\n","    NEEDS TO BE CORRECTED\n","\n","    \"\"\"\n","    def __init__(self,input_channel, output_channel, kernel_size = 3, output = True):\n","        super().__init__()\n","        self.input_channel = input_channel\n","        self.output_channel = output_channel\n","        self.kernel_size = kernel_size\n","        self.output = output\n","        self.model = nn.Sequential( ConvUp(self.input_channel,self.input_channel,self.kernel_size, output))\n","        \n","    def forward(self,x):\n","        # TODO :  Send the data through the model and return the output\n","        output = ...\n","        return"]},{"cell_type":"markdown","metadata":{"id":"IuUrSK7QfURa"},"source":["### iii - The Conv AutoEncoder: final model \n","\n","We stack encoder and decoder in order to form the autoencoder.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGslcjwafURa"},"outputs":[],"source":["# TODO : Stack the Encoder and the Decoder to Create the AE\n","# TODO : Initialize the class attributes\n","# TODO : In the forward method, send the input x through the layers. \n","# Be careful to respect the init attributes of the called class\n","\n","class AutoEncoder_Conv(nn.Module):\n","\n","    def __init__(self,input_size,latent_size, output= True):\n","        super().__init__()\n","        self.input_size = \n","        self.latent_size = \n","        self.output = \n","        self.model = \n","\n","    def forward(self,x):\n","        return \n"]},{"cell_type":"markdown","metadata":{"id":"SxIt_IxgfURa"},"source":["## b - Training and Testing\n","\n","We can reuse the previously written code. However, we need to make some changes...\n","\n","\n","* What changes must we do?\n","* After making these changes, train your model for a latent size of 128.\n","* Test your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUO71k8zBwCI"},"outputs":[],"source":["# TODO : Change your model \n","model = AutoEncoder_Conv(input_size = ,\n","                         latent_size =)\n","\n","# TODO : Reload your HyperParameters\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","net = model.to(device)\n","criterion = ...\n","learning_rate = ...\n","optimizer = ...\n","num_epochs = ...\n","\n","# TODO : Rewrite your Training and Validation Loop\n","for epoch in range(num_epochs) : \n","    running_loss_t,running_loss_v = 0.0, 0.0\n","    for i, data in enumerate(train_loader, 0):\n","\n","    with torch.no_grad():\n","      for i, data in enumerate(val_loader, 0):\n","\n","\n","# TODO : Rewrite your Testing Loop\n","with torch.no_grad():\n","  running_loss = []\n","  for i, data in enumerate(test_loader, 0):  \n","\n","\n","# TODO :  Plot the last batch and the Reconstruction Errors"]},{"cell_type":"markdown","metadata":{"id":"NFdBGGd3fURa"},"source":["Let's compare the results with different sizes (512, 128, 16, 1):\n","* What are the reconstruction values on the test set? \n","* What is the training time?\n","* Which model would you advice? "]},{"cell_type":"markdown","metadata":{"id":"QODNCGtvbvGc"},"source":["## c - Deeper Models\n","\n","All the models we've created are only composed of 3 layers (not really deep): \n","\n","      input layer => hidden layer => output layer\n","\n","We can obviously stack more layers :    \n","* Modify the Convolutional Encoder and the Decoder so that the AE becomes :     \n","      input layer => hidden layer => hidden layer => hidden layer => output layer\n","\n","\n","We want the first and last hidden layer to have the same size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_qBhpBxcjsp"},"outputs":[],"source":["# TODO : Modify the Encoder and the Decoder by adding one additional hidden layer. The size of the additionnal hidden layer can be greater than the previous one\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    Conv Encoder Class\n","    \"\"\"\n","    def __init__(self,input_channel, output_channel, kernel_size = 3):\n","        super().__init__()\n","        self.input_channel = input_channel\n","        self.output_channel = output_channel\n","        self.kernel_size = kernel_size\n","        self.model = ...\n","\n","    def forward(self,x):\n","        # TODO :  Send the data through the model and return the output\n","        output = ...\n","        return\n"," \n","class Decoder(nn.Module):\n","    \"\"\"\n","    Conv Decoder Class\n","    Be careful with the output attribute \n","    \"\"\"\n","    def __init__(self,input_channel, output_channel, kernel_size = 3, output = True):\n","        super().__init__()\n","        self.input_channel = input_channel\n","        self.output_channel = output_channel\n","        self.kernel_size = kernel_size\n","        self.output = output\n","        self.model= ...\n","\n","                    \n","    def forward(self,x):\n","        # TODO :  Send the data through the model and return the output\n","        output = ...\n","        return\n","\n","\n","\n","# TODO : Recreate the AutoEncoder using the Encoder and Decoder \n","class AutoEncoder_Conv(nn.Module):\n","\n","    def __init__(self,input_size,latent_size, output= True):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.latent_size = latent_size\n","        self.output = output\n","        self.model = ...\n","\n","\n","    def forward(self,x):\n","        # TODO :  Send the data through the model and return the output\n","        output = ...\n","        return\n","\n","\n","\n","# TODO : Print the new model\n","print(AutoEncoder_Conv(input_size= ,\n","                       latent_size=))\n"]},{"cell_type":"markdown","metadata":{"id":"_fgaTc96eSA7"},"source":["Retrain your Model for a Compressed Space of :       \n","* 128 \n","* 64 \n","\n","Compare the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kr48TkqhArJx"},"outputs":[],"source":["# TODO : Change your model \n","model = \n","\n","# TODO : Reload your HyperParameters\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","net = model.to(device)\n","criterion = ...\n","learning_rate = ....\n","optimizer = ....\n","num_epochs =...\n","# TODO : Rewrite your Training  and Validation Loop\n","for epoch in range(num_epochs) : \n","    running_loss_t,running_loss_v = 0.0, 0.0\n","    for i, data in enumerate(train_loader, 0):\n","\n","    with torch.no_grad():\n","      for i, data in enumerate(val_loader, 0):\n","\n","# TODO : Rewrite your Testing Loop\n","with torch.no_grad():\n","  running_loss = []\n","  for i, data in enumerate(test_loader, 0):  \n","\n","# TODO :  Plot the last batch and the Reconstruction Errors\n"]},{"cell_type":"markdown","metadata":{"id":"HHkG7AzeIUNL"},"source":["# 5 - Where's that noise?\n","\n","AE can be used to denoise image. It means that if there are some noise in the input image, the AE will be able to reconstruct the image without noise.\n","\n","<img src=\"https://miro.medium.com/max/1400/1*z7SUcHkWp7jT1D_SqvTvgA.png\" height=300>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FTrNKuaI8s9"},"outputs":[],"source":["# TODO : Using torch.randn_like, create some noise and return a noisy input\n","def add_noise(inputs, noise_factor):\n","     noise = \n","     return  noise + ....\n","\n","\n","# TODO : Pick an image from the test set and add noise to it \n","test_image = ... # Pick an image from test dataset\n","test_image = ... # Add noise\n","\n","# TODO : Send the Image through your model and plot the original image and the infered image\n","denoised_image = model(...)\n","fig,axarr = plt.subplots(1, 2)\n","axarr[0].imshow(test_image.squeeze(0).squeeze(0).squeeze(0))\n","axarr[1].imshow(denoised_image.detach().cpu().squeeze(0).squeeze(0).squeeze(0))\n"]},{"cell_type":"markdown","metadata":{"id":"wKdCjPuwMhxA"},"source":["\n","* Can you explain why the AE is efficient for denoising images ? \n","* Does it work with lots of noise ?"]},{"cell_type":"markdown","metadata":{"id":"sbc21j0pezi4"},"source":["# 6 - Getting some colors (optional)\n","\n","In this part, you will try all the things you've seen previously on a new Dataset. Instead of black and white images, here we have RGB colored images.\n","\n","You must :    \n","* Check your Data\n","* Create the Dataloaders\n","* Create your Models\n","* Train your model for different parameters\n","* Test the model\n","* Provide some quantitative results on the behavior of your models.\n"]},{"cell_type":"markdown","metadata":{"id":"mbY6LTflOZgg"},"source":["## CIFAR10 : Colors and Classes\n","\n","\n","Do the similar steps as before. \n","\n","- What is the size of the train dataset?\n","- What are the elements available in one piece of data? (image,label)\n","- What is the shape of one piece of data?\n","- What is the type of one piece of data?\n","- Plot few elements of the dataset using Matplotlib.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBRmB5DUD3wB"},"outputs":[],"source":["transform = transforms.Compose([transforms.ToTensor()])\n","dataset_train = CIFAR10(root='./data', train=True,download=True, transform=transform)\n","dataset_test =  CIFAR10(root='./data', train=False,download=True, transform=transform)\n","# TODO : Do all the things related to DATA here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYBJkBudD9S8"},"outputs":[],"source":["# TODO : Do all the things related to MLP MODELs here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x68ewYEUEBBF"},"outputs":[],"source":["# TODO : Do all the things related to TRAINING here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htf5ZZsWdoOg"},"outputs":[],"source":["# TODO : Do all the things related to TESTING here"]},{"cell_type":"markdown","metadata":{"id":"RXFXKZZQEJSe"},"source":["Don't forget to write your results and explanation in your report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLeYUiRsEGCd"},"outputs":[],"source":["# TODO : Do all the things related to Conv MODELs here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zbLOzXCEGCd"},"outputs":[],"source":["# TODO : Do all the things related to TRAINING here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qL59wDFkdoOh"},"outputs":[],"source":["# TODO : Do all the things related to TESTING here"]},{"cell_type":"markdown","metadata":{"id":"_274sHzHETsk"},"source":["Don't forget to write your results and explanation in your report"]},{"cell_type":"markdown","metadata":{"id":"aYj-oU52PGK8"},"source":["# 7 - Results\n","\n","Now analyze your results on the coloured Dataset.\n","* Is it much harder than the B&W Dataset ?\n","* Is the model efficient ?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"D64B12YFdoOh"},"source":["* Sum up all your understanding on this subject your report.\n","\n","You should explain:\n","* The dataset you used for training,\n","* The tests you did,\n","* The models you tested,\n","* The results you had,\n","* The explanation of the results (why it works, why it doesn't),\n","* How can we compute the compression rate of the model? \n","* Do we need to consider the model's size?"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of AE_Eleve.ipynb","provenance":[{"file_id":"https://github.com/thad75/TP_ENSEA_ELEVE/blob/main/2A/Majeure%20Signal/Image%20Compression%20using%20Deep%20Learning.ipynb","timestamp":1647852029401}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
